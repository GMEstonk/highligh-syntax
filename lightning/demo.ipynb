{

"cells": [

  {

   "cell_type": "markdown",

   "id": "832da195-0ce1-49ab-a76a-46b18b2807e7",

   "metadata": {

    "execution": {

     "iopub.execute_input": "2023-07-10T13:14:27.032551Z",

     "iopub.status.busy": "2023-07-10T13:14:27.032234Z",

     "iopub.status.idle": "2023-07-10T13:14:27.038986Z",

     "shell.execute_reply": "2023-07-10T13:14:27.038403Z",

     "shell.execute_reply.started": "2023-07-10T13:14:27.032533Z"

    }

   },

   "source": [

    "**Install and import needed libraries**"

   ]

  },

  {

   "cell_type": "code",

   "execution_count": null,

   "id": "9736b83e-e74b-4fbf-bbe7-a5c4a4d33cb8",

   "metadata": {

    "collapsed": true,

    "execution": {

     "iopub.execute_input": "2023-07-07T16:11:36.697084Z",

     "iopub.status.busy": "2023-07-07T16:11:36.696859Z",

     "iopub.status.idle": "2023-07-07T16:15:52.682665Z",

     "shell.execute_reply": "2023-07-07T16:15:52.681913Z",

     "shell.execute_reply.started": "2023-07-07T16:11:36.697067Z"

    },

    "jupyter": {

     "outputs_hidden": true

    },

    "tags": []

   },

   "outputs": [],

   "source": [

    "!pip install transformers\n",

    "!pip install torch\n",

    "!pip install einops\n",

    "!pip install accelerate\n",

    "!pip install bitsandbytes\n",

    "!pip install xformers\n",

    "!pip install chromadb"

   ]

  },

  {

   "cell_type": "code",

   "execution_count": null,

   "id": "ecfa2dc7-5e47-456d-96b0-f4b40ed8771e",

   "metadata": {

    "execution": {

     "iopub.execute_input": "2023-07-07T16:15:52.684185Z",

     "iopub.status.busy": "2023-07-07T16:15:52.683999Z",

     "iopub.status.idle": "2023-07-07T16:15:54.505419Z",

     "shell.execute_reply": "2023-07-07T16:15:54.504609Z",

     "shell.execute_reply.started": "2023-07-07T16:15:52.684166Z"

    },

    "tags": []

   },

   "outputs": [],

   "source": [

    "import chromadb\n",

    "import pandas as pd\n",

    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",

    "import transformers\n",

    "import torch\n",

    "from ipywidgets import widgets, Layout, Box, HBox, VBox\n",

    "from IPython.display import display, clear_output\n",

    "import threading"

   ]

  },

  {

   "cell_type": "markdown",

   "id": "3a81915a-918f-4400-b0b5-27c3161571c3",

   "metadata": {},

   "source": [

    "**Create ChromaDB client and insert data**"

   ]

  },

  {

   "cell_type": "code",

   "execution_count": null,

   "id": "25fceb1f-6646-4046-9464-1b66e57c8c0f",

   "metadata": {

    "execution": {

     "iopub.execute_input": "2023-07-07T16:16:38.440699Z",

     "iopub.status.busy": "2023-07-07T16:16:38.439927Z",

     "iopub.status.idle": "2023-07-07T16:16:38.586727Z",

     "shell.execute_reply": "2023-07-07T16:16:38.585937Z",

     "shell.execute_reply.started": "2023-07-07T16:16:38.440674Z"

    },

    "tags": []

   },

   "outputs": [],

   "source": [

    "client = chromadb.Client()\n",

    "collection = client.create_collection(\"go_links\")"

   ]

  },

  {

   "cell_type": "code",

   "execution_count": null,

   "id": "a7bfc7e7-8b54-4ddf-8635-8b7af0b19fa3",

   "metadata": {

    "execution": {

     "iopub.execute_input": "2023-07-07T16:16:40.532327Z",

     "iopub.status.busy": "2023-07-07T16:16:40.531479Z",

     "iopub.status.idle": "2023-07-07T16:16:42.318058Z",

     "shell.execute_reply": "2023-07-07T16:16:42.317179Z",

     "shell.execute_reply.started": "2023-07-07T16:16:40.532302Z"

    },

    "tags": []

   },

   "outputs": [],

   "source": [

    "df = pd.read_csv(r\"index_without_people.csv\") #Created in CreateGoSites.ipynb\n",

    "df.columns = [\"DROP\"] + list(df.columns)[1:] #Don't judge me, I struggled to get rid of an annoying index column and went with this.\n",

    "df = df.drop(\"DROP\",axis=1)\n",

    "\n",

    "records = df.to_dict(\"records\") #creates a list of [{colName: row1Val, colName2: row1val}]\n",

    "documents = [i[\"description\"] for i in records] #Getting the data formatted for Chroma\n",

    "ids = [i[\"go_link\"] for i in records]\n",

    "metadata = [{\"type\":i[\"type\"],\"content\":i[\"content\"], \"approx_description_tokens\":len(i[\"description\"]), \"approx_content_tokens\": len(str(i[\"content\"]))} for i in records]\n",

    "\n",

    "#before adding, make sure the data is present and there are no missing docs, ids or metadata\n",

    "if len(documents) == len(metadata) and len(documents) == len(ids) and len(documents) != 0:\n",

    "    collection.add(documents = documents, metadatas = metadata, ids = ids)\n",

    "else:\n",

    "    print(\"ERROR IMPORTING GO SITES\")"

   ]

  },

  {

   "cell_type": "markdown",

   "id": "8c06654b-637f-4294-9e29-341ce712032a",

   "metadata": {},

   "source": [

    "**Configure Model**"

   ]

  },

  {

   "cell_type": "code",

   "execution_count": null,

   "id": "b36ef284-dfac-4133-a7a2-fafa7a8fcc64",

   "metadata": {

    "collapsed": true,

    "execution": {

     "iopub.execute_input": "2023-07-07T16:16:42.319452Z",

     "iopub.status.busy": "2023-07-07T16:16:42.319160Z",

     "iopub.status.idle": "2023-07-07T16:19:46.092112Z",

     "shell.execute_reply": "2023-07-07T16:19:46.091277Z",

     "shell.execute_reply.started": "2023-07-07T16:16:42.319435Z"

    },

    "jupyter": {

     "outputs_hidden": true

    },

    "tags": []

   },

   "outputs": [],

   "source": [

    "#https://huggingface.co/blog/falcon is where I got the most of the code for this section, and is a great resource for learning about this model\n",

    "\n",

    "model_id = \"tiiuae/falcon-40b-instruct\"\n",

    "\n",

    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",

    "print(\"created tokenizer\")\n",

    "model = AutoModelForCausalLM.from_pretrained(\n",

    "    model_id,\n",

    "    torch_dtype=torch.bfloat16,\n",

    "    trust_remote_code=True,\n",

    "    load_in_8bit=True, #keeps it under the 80GB of vram on Caspian (albeit with slightly reduced performance)\n",

    "    device_map=\"auto\",\n",

    ")\n",

    "print(\"created model\")\n",

    "pipeline = transformers.pipeline(\n",

    "    \"text-generation\",\n",

    "    model=model,\n",

    "    tokenizer=tokenizer,\n",

    ")\n",

    "print(\"created pipeline\")"

   ]

  },

  {

   "cell_type": "markdown",

   "id": "7fe43235-5ffa-4f3c-9f84-6a5e8f4a3a65",

   "metadata": {},

   "source": [

    "**Define get_summary function**\n",

    "\n",

    "This function is used to help pack information into the limited context window of Falcon. This is a major weak area of my code (largely because it was completed shortly before I presented) so if you are looking for something to experiment with this is a good place. Honestly Falcon should not even be the model to do the summary generation. "

   ]

  },

  {

   "cell_type": "code",

   "execution_count": null,

   "id": "6da55577-f36c-4a3b-a80d-bb4d67dc1dd5",

   "metadata": {

    "execution": {

     "iopub.execute_input": "2023-07-07T16:19:46.093725Z",

     "iopub.status.busy": "2023-07-07T16:19:46.093240Z",

     "iopub.status.idle": "2023-07-07T16:19:46.100329Z",

     "shell.execute_reply": "2023-07-07T16:19:46.099747Z",

     "shell.execute_reply.started": "2023-07-07T16:19:46.093705Z"

    },

    "tags": []

   },

   "outputs": [],

   "source": [

    "def get_summary(text, question, max_tokens):\n",

    "    # If the text length (chars) is shorter than max tokens, return the text, if max tokens is too high, reduce it.\n",

    "    if max_tokens > 1700:\n",

    "        max_tokens = 1700\n",

    "    if len(text) <= max_tokens:\n",

    "        return text\n",

    "\n",

    "    #break the text into fragments so that the text to be summarized does not exceed the token limit of Falcon\n",

    "    fragments = []\n",

    "    if len(text) > 1700-max_tokens:\n",

    "        for i in range(0, len(text), int(1700-max_tokens)):\n",

    "            fragments.append(text[i:i+int(1700-max_tokens)])\n",

    "    else:\n",

    "        fragments.append(text)\n",

    "\n",

    "    # Summarize each fragment and concatenate the summaries\n",

    "    summaries = []\n",

    "    total_tokens = 0\n",

    "    for fragment in fragments:\n",

    "        #prompt can be worked on, set up as an effort to not loose critical info but often produces poor results. \n",

    "        nonlimiting = \"SUMMARIZE THE TEXT, IF INFORMATION REGARDING THE QUESTION IS PRESENT, INCLUDE IT, OTHERWISE JUST SUMMARIZE NORMALLY. DO NOT INCLUDE ANY INFORMATION NOT PRESENT IN THE DOCUMENT YOU ARE SUMMARIZING. \\n QUESTION: \"\n",

    "        prompt = nonlimiting + question + \\\n DOCUMENT: \ + fragment + \\\n SUMMARY: \\n",

    "        print(len(prompt))\n",

    "        sequences = pipeline(\n",

    "            prompt,\n",

    "            max_new_tokens=max_tokens/len(fragments),\n",

    "            do_sample=True,\n",

    "            top_k=10,\n",

    "            num_return_sequences=1,\n",

    "            eos_token_id=tokenizer.eos_token_id,\n",

    "        )\n",

    "        print(sequences[0]['generated_text'].replace(prompt, \"\"))\n",

    "        summary = sequences[0]['generated_text'].replace(prompt,\"\")\n",

    "        summary_tokens = len(summary)\n",

    "\n",

    "        # If adding this summary would exceed the total token limit, crop it\n",

    "        if total_tokens + summary_tokens > max_tokens:\n",

    "            remaining_tokens = max_tokens-(total_tokens + summary_tokens)\n",

    "            summaries.append(summary[:int(remaining_tokens)]) #Poor bug fix implemented in time crunch. This solution is near the top of the list of things to fix (if I don't get frustrated and redo this whole section)\n",

    "            total_tokens += summary_tokens\n",

    "            break\n",

    "\n",

    "        # Add the summary to the list and update the total token count\n",

    "        summaries.append(summary)\n",

    "        total_tokens += summary_tokens\n",

    "\n",

    "    # Concatenate the summaries\n",

    "    final_summary = ' '.join(summaries)\n",

    "\n",

    "    return final_summary\n"

   ]

  },

  {

   "cell_type": "markdown",

   "id": "1c9c1789-aa72-41f0-92db-9070c33d633a",

   "metadata": {},

   "source": [

    "**Defines the get_documents function**\n",

    "\n",

    "This function queries chroma for pages with a description that is semantically similar to the prompt. It retrieves as many documents as it can up until the limit, and never passing the token limit. "

   ]

  },

  {

   "cell_type": "code",

   "execution_count": null,

   "id": "affe0a55-4cc2-4e4b-b2c9-ab10edee7535",

   "metadata": {

    "execution": {

     "iopub.execute_input": "2023-07-07T16:43:11.459724Z",

     "iopub.status.busy": "2023-07-07T16:43:11.459478Z",

     "iopub.status.idle": "2023-07-07T16:43:11.467058Z",

     "shell.execute_reply": "2023-07-07T16:43:11.466533Z",

     "shell.execute_reply.started": "2023-07-07T16:43:11.459707Z"

    },

    "tags": []

   },

   "outputs": [],

   "source": [

    "def get_documents(query, num_docs=1, type=\"all\", max_tokens=1500):\n",

    "    # Query the collection\n",

    "    result = collection.query(query_texts=[query], n_results=num_docs)\n",

    "    # Initialize a list to store the final documents\n",

    "    final_docs = []\n",

    "\n",

    "    # Initialize counters for the total number of tokens\n",

    "    total_tokens = 0\n",

    "\n",

    "    # Initialize a list to store 'i' type documents and their indices\n",

    "    i_docs = []\n",

    "    \n",

    "    # Single pass: process all documents\n",

    "    for i in range(len(result['ids'][0])):\n",

    "        id = result['ids'][0][i]\n",

    "        doc = result['documents'][0][i]\n",

    "        metadata = result['metadatas'][0][i]\n",

    "        if metadata['type'] == 't':\n",

    "            content = metadata['content']\n",

    "            content_tokens = len(content) \n",

    "\n",

    "            # If adding this document would exceed the total token limit, discard it\n",

    "            if total_tokens + content_tokens > max_tokens:\n",

    "                continue\n",

    "\n",

    "            # Add the document to the final list and update the total token count\n",

    "            final_docs.append((id, content))\n",

    "            total_tokens += content_tokens\n",

    "        elif metadata['type'] == 'i':\n",

    "            # Store 'i' type document for later processing\n",

    "            i_docs.append((i, id, doc, metadata))\n",

    "        elif metadata['type'] not in ['t', 'i']:\n",

    "            doc_tokens = len(doc.split())  # Assuming the document's token count is its word count\n",

    "\n",

    "            if total_tokens + doc_tokens > max_tokens:\n",

    "                final_docs.append((id, doc[:max_tokens-(total_tokens+doc_tokens)])) #Very temporary fix to a more fundimental problem\n",

    "                continue\n",

    "\n",

    "            # Add the document to the final list and update the total token count\n",

    "            final_docs.append((id, doc))\n",

    "            total_tokens += doc_tokens\n",

    "\n",

    "    # Calculate the maximum tokens for 'i' type document summaries\n",

    "    summary_max_tokens = max_tokens/num_docs\n",

    "    \n",

    "    # Process 'i' type documents in their original order\n",

    "    for (i, id, doc, metadata) in i_docs:\n",

    "        content = metadata['content']\n",

    "        # Summarize the content\n",

    "        content = get_summary(content, query, summary_max_tokens)\n",

    "        content_tokens = len(content)\n",

    "\n",

    "        # If adding this document would exceed the total token limit, discard it\n",

    "        if total_tokens + content_tokens > max_tokens:\n",

    "            remaining_tokens = max_tokens-(total_tokens + content_tokens)\n",

    "            final_docs.insert(i, (id, content[:remaining_tokens]))  # Insert at the original index\n",

    "            total_tokens += content_tokens\n",

    "            continue\n",

    "\n",

    "        # Add the document to the final list and update the total token count\n",

    "        final_docs.insert(i, (id, content))  # Insert at the original index\n",

    "        total_tokens += content_tokens\n",

    "\n",

    "    return final_docs\n"

   ]

  },

  {

   "cell_type": "markdown",

   "id": "bf3e2e4a-91a5-44bb-8751-5a3c5d556411",

   "metadata": {},

   "source": [

    "**The get_response function is a simple text completion with basic prompt engineering.**\n",

    "\n",

    "Has room to be improved. Beware of using to much of the prompting space. "

   ]

  },

  {

   "cell_type": "code",

   "execution_count": null,

   "id": "4b3f7470-9e82-4bfe-8a7c-ad87a26310a1",

   "metadata": {

    "execution": {

     "iopub.execute_input": "2023-07-07T16:51:04.924928Z",

     "iopub.status.busy": "2023-07-07T16:51:04.924520Z",

     "iopub.status.idle": "2023-07-07T16:51:04.929777Z",

     "shell.execute_reply": "2023-07-07T16:51:04.929020Z",

     "shell.execute_reply.started": "2023-07-07T16:51:04.924903Z"

    },

    "tags": []

   },

   "outputs": [],

   "source": [

    "def get_response(question, context):\n",

    "    nonlimiting = \"ANSWER WITH HOW THE USER CAN SOLVE THEIR PROBLEM BASED ON THE PROVIDED CONTEXT. PULL FROM THE CONTEXT ABOVE YOUR OWN UNDERSTANDING AND KNOWLEDGE.\"\n",

    "    prompt = \"CONTEXT: \\n\" + context + \\\n QUESTION \\n\ + question + \\\n\ + nonlimiting + \\\n ANSWER \\n\\n",

    "    if len(prompt) > 1500:\n",

    "        prompt = \"CONTEXT: \\n\" + context[:-1500+len(prompt)] + \\\n QUESTION \\n\ + question + \\\n\ + nonlimiting + \\\n ANSWER \\n\\n",

    "    sequences = pipeline(\n",

    "        prompt,\n",

    "        max_new_tokens=500,\n",

    "        max_length=2000,\n",

    "        do_sample=True,\n",

    "        top_k=10,\n",

    "        num_return_sequences=1,\n",

    "        eos_token_id=tokenizer.eos_token_id,\n",

    "    )\n",

    "    return sequences[0]['generated_text'].replace(prompt, \"\")"

   ]

  },

  {

   "cell_type": "markdown",

   "id": "6d837af0-b427-4666-aca2-9c847ee3bc35",

   "metadata": {},

   "source": [

    "**Here is the actual chat window**\n",

    "\n",

    "It uses ipywidgets to create a simple chat interface with HTML based formatting, it is not the cleanest thing in the world but it seemed easier than setting up an API and hosting a webpage. "

   ]

  },

  {

   "cell_type": "code",

   "execution_count": null,

   "id": "c7728a65-0f08-431e-8c41-a3341d21379b",

   "metadata": {

    "execution": {

     "iopub.execute_input": "2023-07-07T16:54:42.808991Z",

     "iopub.status.busy": "2023-07-07T16:54:42.808623Z",

     "iopub.status.idle": "2023-07-07T16:54:42.826056Z",

     "shell.execute_reply": "2023-07-07T16:54:42.825416Z",

     "shell.execute_reply.started": "2023-07-07T16:54:42.808967Z"

    },

    "tags": []

   },

   "outputs": [],

   "source": [

    "conversation = \"ASSISTANT: Hello! How can I help you?\\n\"\n",

    "\n",

    "def format_conversation(conversation):\n",

    "    formatted_conversation = conversation.replace(\"USER:\", \"<b style='color:blue'>USER:</b>\")\n",

    "    formatted_conversation = formatted_conversation.replace(\"ASSISTANT:\", \"<b style='color:red'>ASSISTANT:</b>\")\n",

    "    formatted_conversation = formatted_conversation.replace(\\\n\,\"<br>\")\n",

    "    return formatted_conversation\n",

    "\n",

    "def response_and_update(user_input):\n",

    "    global conversation\n",

    "    documents = get_documents(user_input, max_tokens=1400, num_docs=1)\n",

    "    go_links = \", \".join([\"go/\" + str(x[0]) for x in documents])\n",

    "    context = \" \".join([x[1] for x in documents])\n",

    "    response = get_response(conversation[47:], context)\n",

    "    conversation += f\"ASSISTANT: {response[1:]}<br>ASSISTANT: For more information, check out {go_links}<br>\"\n",

    "    chat_history.value = format_conversation(conversation)\n",

    "    user_text.value = \"\"\n",

    "    with output:\n",

    "        clear_output()\n",

    "\n",

    "def on_send_button_click(b):\n",

    "    global conversation\n",

    "    user_input = user_text.value\n",

    "    conversation += f\"USER: {user_input}<br>\"\n",

    "    chat_history.value = format_conversation(conversation)\n",

    "    with output:\n",

    "        display(widgets.HTML(value=\"<i style='color:blue'>Thinking...</i>\"))\n",

    "    thread = threading.Thread(target=response_and_update, args=(user_input,))\n",

    "    thread.start()\n",

    "\n",

    "chat_history = widgets.HTML(\n",

    "    value=format_conversation(conversation),\n",

    "    layout=Layout(width='100%', height='300px', overflow_y='auto')\n",

    ")\n",

    "\n",

    "user_text = widgets.Text(\n",

    "    value='',\n",

    "    placeholder='Type something',\n",

    "    layout=Layout(width='85%')\n",

    ")\n",

    "\n",

    "send_button = widgets.Button(\n",

    "    description='Send',\n",

    "    layout=Layout(width='15%')\n",

    ")\n",

    "send_button.on_click(on_send_button_click)\n",

    "\n",

    "output = widgets.Output()\n",

    "\n",

    "# Place conversation and output in one VBox\n",

    "conversation_box = VBox([chat_history, output])\n",

    "\n",

    "# Create another box for user_text and send_button\n",

    "input_box = HBox([user_text, send_button])\n",

    "\n",

    "# Combine the two boxes\n",

    "chat_box = VBox([conversation_box, input_box])\n",

    "\n",

    "display(chat_box)\n"

   ]

  },

  {

   "cell_type": "markdown",

   "id": "ad1f352b-5012-4929-989b-993e868c038a",

   "metadata": {},

   "source": [

    "If you have any questions, improvements, feedback or anything at all, just drop me a Slack :-)\n",

    "\n",

    "-Joey Hersh"

   ]

  }

],

"metadata": {

  "interpreter": {

   "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"

  },

  "kernelspec": {

   "display_name": "Python 3.9.5 64-bit",

   "language": "python",

   "name": "python3"

  },

  "language_info": {

   "codemirror_mode": {

    "name": "ipython",

    "version": 3

   },

   "file_extension": ".py",

   "mimetype": "text/x-python",

   "name": "python",

   "nbconvert_exporter": "python",

   "pygments_lexer": "ipython3",

   "version": "3.9.5"

  }

},

"nbformat": 4,

"nbformat_minor": 5

}
